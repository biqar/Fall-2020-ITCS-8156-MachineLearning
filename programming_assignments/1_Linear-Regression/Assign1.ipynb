{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #1 - Linear Regression\n",
    "\n",
    "<font color=\"blue\"> Abdullah Al Raqibul Islam </font>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INDEX\n",
    "\n",
    "I. Introduction<br/>\n",
    "II. Data<br/>\n",
    "&emsp;&emsp;II.I. Reading the data<br/>\n",
    "&emsp;&emsp;II.II. Preprocessing of the data<br/>\n",
    "&emsp;&emsp;II.III. Visualization of the data<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;II.III.I. Correlation Heatmap<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;II.III.II. Pie-chart<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;II.III.III. Count Plots<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;II.III.IV. Line Plots<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;II.III.V. Linear Regression Plots<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;II.III.VI. Box Plots<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;II.III.VII. Preliminary Observation About Data<br/>\n",
    "III. Method<br/>\n",
    "&emsp;&emsp;III.I. Super Classs Definition<br/>\n",
    "&emsp;&emsp;III.II. Least Squares<br/>\n",
    "&emsp;&emsp;III.III. Least Mean Squares<br/>\n",
    "IV. Usage Examples<br/>\n",
    "&emsp;&emsp;IV.I. Usage Example: Least Squares<br/>\n",
    "&emsp;&emsp;IV.II. Usage Example: Least Mean Squares<br/>\n",
    "V. Preliminary Test<br/>\n",
    "VI. Experiments<br/>\n",
    "&emsp;&emsp;VI.I. Plotting Functions<br/>\n",
    "&emsp;&emsp;VI.II. Data Partitioning<br/>\n",
    "&emsp;&emsp;**VI.III. Experiments on Least Squares**<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;VI.III.I. Feature Scaling: Unscaled Data<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.III.I.I. Feature Selection: All Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.III.I.II. Feature Selection: Significant Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;VI.III.II. Feature Scaling: Normalizaed Data<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.III.II.I. Feature Selection: All Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.III.II.II. Feature Selection: Significant Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;VI.III.III. Feature Scaling: Standarized Data<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.III.III.I. Feature Selection: All Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.III.III.II. Feature Selection: Significant Features<br/>\n",
    "&emsp;&emsp;**VI.IV. Experiments on Least Mean Squares (LMS)**<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;VI.IV.I. Feature Scaling: Unscaled Data<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.IV.I.I. Feature Selection: All Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.IV.I.II. Feature Selection: Significant Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;VI.IV.II. Feature Scaling: Normalized Data<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.IV.II.I. Feature Selection: All Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.IV.II.II. Feature Selection: Significant Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;VI.IV.III. Feature Scaling: Standarized Data<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.IV.III.I. Feature Selection: All Features<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;VI.IV.III.II. Feature Selection: Significant Features<br/>\n",
    "VII. Conclusions<br/>\n",
    "VIII. Extra Credit<br/>\n",
    "IX. References<br/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I. Introduction\n",
    "\n",
    "The goal of this assignment is to perform linear regression on a dataset to predict the price of house sale [[1]](https://www.kaggle.com/harlfoxem/housesalesprediction) based on the different features (i.e. size, condition, facility).\n",
    "\n",
    "The features have been explained in the data section. I have implemented and used two linear regression models namely `Least Squares` and `Least Mean Squares`. The detail of the algorithm discussed in `section III`. Section `IV` and `V` contains the integraty testing of the implemented algorithms. In `section VI`, I discussed the performance evaluations in details of the implementation domain. This section also contains the extra credit works listed in `section VIII`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# II. Data\n",
    "\n",
    "In this expriment I used `House Sales in King County, USA` [[1]](https://www.kaggle.com/harlfoxem/housesalesprediction) dataset from `kaggle` [[2]](https://www.kaggle.com). This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. The dataset have 21613 rows and 21 attributes. Here is the attribute list along with the data-type.\n",
    "\n",
    "**Attribute Information:**\n",
    " 0.   id (int64)\n",
    " 1.   date (object)\n",
    " 2.   price (float64)\n",
    " 3.   bedrooms (int64)\n",
    " 4.   bathrooms (float64)\n",
    " 5.   sqft_living (int64)\n",
    " 6.   sqft_lot (int64)\n",
    " 7.   floors (float64)\n",
    " 8.   waterfront (int64)\n",
    " 9.   view (int64)\n",
    " 10.  condition (int64)\n",
    " 11.  grade (int64)\n",
    " 12.  sqft_above (int64)\n",
    " 13.  sqft_basement (int64)\n",
    " 14.  yr_built (int64)\n",
    " 15.  yr_renovated (int64)\n",
    " 16.  zipcode (int64)\n",
    " 17.  lat (float64)\n",
    " 18.  long (float64)\n",
    " 19.  sqft_living15 (int64)\n",
    " 20.  sqft_lot15 (int64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II.I. Reading the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# reading the data from csv datafile; using read_csv method from Pandas library\n",
    "df_r = pd.read_csv(\"data/regression/kc_house_data.csv\")\n",
    "\n",
    "# displaying all the columns data of top 5 rows in the jupyter notebook\n",
    "pd.set_option('max_columns', 28)\n",
    "df_r.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_r.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II.II. Preprocessing of the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the metadata; getting familiarized with columns and data-types\n",
    "df_r.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the \"Non-Null Count\" column above, we can observe all the rows contain same number of `non-null` counts. From this count we can sense that there is no null values in this dataset.\n",
    "\n",
    "To confirm this, let's check it using library function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking columns with null values\n",
    "df_r.isna().any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is no null values in any particular columns. So we don't need to perform any data pre-processing here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# observe pairwise correlation of columns using library function\n",
    "df_r.corr(method='pearson')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pandas dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe. Non-numeric data type columns from the dataframe is ignored (i.e. \"date\" column in this case).\n",
    "\n",
    "This correlation matrix gives the measure of the strength of the association between the two columns. I used Pearson's Correlation Coefficient method to generate this matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II.III. Visualization of the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.III.I. Correlation Heatmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(df_r.corr(method='pearson'), annot=True, linewidths=.1,fmt= '.1f')\n",
    "plt.title(\"Correlation Heatmap for HouseSales Data\", color = 'green', fontsize = 20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation:\n",
    "\n",
    "1. The most correlated attributes with `price` are\n",
    "    * bathrooms\n",
    "    * sqft_living\n",
    "    * view\n",
    "    * grade\n",
    "    * sqft_above\n",
    "    * sqft_living15\n",
    "2. Among them all, `sqft_living` and `grade` have the highest correlation coefficient (`0.7`).\n",
    "3. This actually makes sense, as this data is suitable for `regrassion analysis` and house price mostly depends on the `size` and `quality`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.III.II. Pie-chart"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot number of records in the dataset for different condition\n",
    "\n",
    "conditionList = []\n",
    "conditionCount = []\n",
    "for condition_name, subset in df_r.groupby('condition'):\n",
    "    conditionList.append(condition_name)\n",
    "    conditionCount.append(len(subset))\n",
    "print(conditionList)\n",
    "print(conditionCount)\n",
    "\n",
    "plt.figure(figsize = (7, 7))\n",
    "plt.pie(conditionCount, labels = conditionList)\n",
    "plt.title(\"Pie Chart: Condition\", color = 'green', fontsize = 20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.III.III. Count Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,6))\n",
    "sns.countplot(df_r.bathrooms)\n",
    "plt.title(\"Count Plot: # of bathrooms\", color = 'green', fontsize = 30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot number of records in the dataset for different condition, view, waterfront, bedrooms, grade, zipcode\n",
    "\n",
    "f, axes = plt.subplots(1, 3,figsize=(25,6))\n",
    "sns.countplot(x= \"condition\", data=df_r, orient='v', ax=axes[0])\n",
    "sns.countplot(x= \"view\", data=df_r, orient='v', ax=axes[1])\n",
    "sns.countplot(x= \"waterfront\", data=df_r, orient='v', ax=axes[2])\n",
    "\n",
    "f, axes = plt.subplots(1, 3,figsize=(25,6))\n",
    "sns.countplot(x= \"bedrooms\", data=df_r, orient='v', ax=axes[0])\n",
    "sns.countplot(x= \"grade\", data=df_r, orient='v', ax=axes[1])\n",
    "sns.countplot(x= \"zipcode\", data=df_r, orient='v', ax=axes[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation:\n",
    "\n",
    "1. The `pie-chart` and `count-plot` is used to compare parts to the whole. It helps picturing which types of data exist more in the dataset and can influence making any decision.\n",
    "2. From the pie-chart we can observe that people buy more `condition 3` type house.\n",
    "3. From the count-plot we can make simillar type of observations like we made in the pie-chart. That is majority people likes house with `2.5 bathrooms`.\n",
    "4. Similarly, the group count-plot shows the prefered values for `condition`, `view`, `waterfront`, `bedrooms`, `grade`, and `zipcode`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.III.IV. Line Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the prices based on size\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(x=\"sqft_living\", y=\"price\", data=df_r)\n",
    "plt.title(\"Line Plot: Price Vs. Size\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.III.V. Linear Regression Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot linear regression on price based on size\n",
    "\n",
    "g = sns.lmplot(data=df_r, x=\"sqft_living\", y=\"price\")\n",
    "g.set_axis_labels(\"Living Space (sqft)\", \"Price (usd)\")\n",
    "plt.title(\"Linear Regression Plot: Price Vs. Size\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot linear regression on price based on size15\n",
    "\n",
    "g = sns.lmplot(data=df_r, x=\"sqft_living15\", y=\"price\")\n",
    "g.set_axis_labels(\"Living Space 15 (sqft)\", \"Price (usd)\")\n",
    "plt.title(\"Linear Regression Plot: Price Vs. Size_15\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot linear regression on price based on size_above\n",
    "\n",
    "g = sns.lmplot(data=df_r, x=\"sqft_above\", y=\"price\")\n",
    "g.set_axis_labels(\"Living Space Above (sqft)\", \"Price (usd)\")\n",
    "plt.title(\"Linear Regression Plot: Price Vs. Size_above\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot multiple linear regression on price based on size varying number of bedrooms\n",
    "\n",
    "g = sns.lmplot(data=df_r, x=\"sqft_living\", y=\"price\", hue=\"bedrooms\")\n",
    "g.set_axis_labels(\"Living Space (sqft)\", \"Price (usd)\")\n",
    "plt.title(\"Multiple Linear Regression Plot: Price Vs. Size (w.r.t # of bedrooms)\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot multiple linear regression on price based on size varying waterfront\n",
    "\n",
    "g = sns.lmplot(data=df_r, x=\"sqft_living\", y=\"price\", hue=\"waterfront\")\n",
    "g.set_axis_labels(\"Living Space (sqft)\", \"Price (usd)\")\n",
    "plt.title(\"Multiple Linear Regression Plot: Price Vs. Size (w.r.t waterfront)\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot multiple linear regression on price based on size varying view\n",
    "\n",
    "g = sns.lmplot(data=df_r, x=\"sqft_living\", y=\"price\", hue=\"view\")\n",
    "g.set_axis_labels(\"Living Space (sqft)\", \"Price (usd)\")\n",
    "plt.title(\"Multiple Linear Regression Plot: Price Vs. Size (w.r.t waterfront)\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot multiple linear regression on price based on size varying condition\n",
    "\n",
    "g = sns.lmplot(data=df_r, x=\"sqft_living\", y=\"price\", hue=\"condition\")\n",
    "g.set_axis_labels(\"Living Space (sqft)\", \"Price (usd)\")\n",
    "plt.title(\"Multiple Linear Regression Plot: Price Vs. Size (w.r.t condition)\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation:\n",
    "\n",
    "1. The linear regression plots clearly show correlation between price and size.\n",
    "2. The multiple linear regression plots indicate -\n",
    "    * Price is higher with waterfront\n",
    "    * House with view 4 have highest price within the same living space size\n",
    "    * House with condition 5 price more"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.III.VI. Box Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot price on different condition\n",
    "\n",
    "sns.boxplot(x='condition', y='price', data=df_r)\n",
    "plt.title(\"Box Plot: Price Vs. Condition\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot price on varying bedrooms\n",
    "\n",
    "sns.boxplot(x='bedrooms', y='price', data=df_r)\n",
    "plt.title(\"Box Plot: Price Vs. Bedrooms\", color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation:\n",
    "\n",
    "1. The boxplot indicate -\n",
    "    * Price range varies most for houses with 8 bedrooms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.III.VII. Preliminary Observation About Data\n",
    "\n",
    "We made individual observations on the different plotting above. To summarize\n",
    "\n",
    "* People mostly like houses with the following property -\n",
    "    * Three bedroom\n",
    "    * Condition 3\n",
    "    * Seventh grade\n",
    "    * 2.5 Bathrooms\n",
    "    * Without waterfront\n",
    "    * View 0\n",
    "* This is actually true as we have observed from the linear regression graph that `with waterfront` price is prety high."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# III. Method\n",
    "\n",
    "As I mentioned before, in this experiment I used two linear regression algorithms on the data described in `section II`. Given data, the goal of the linear regression algorithm is to find a best fit on all the data. The equation of a line can be written as\n",
    "\n",
    "$$\n",
    "f(x; a, b) = a x + b.\n",
    "$$\n",
    "\n",
    "Where we can replace the `a` and `b` by the weight symbol $w$,\n",
    "$$\n",
    "f(x; w) =  w_1 x + w_0.\n",
    "$$\n",
    "\n",
    "Considering multiple input features as $x$, we can extend the input $x$ to an input vector with bias input $x_0 = 1$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x; w) &=  w_D x_D + \\cdots + w_1 x_1 + w_0 \\\\\n",
    "            &= \\sum_{i=0}^{D} w_i x_i \\quad\\text{where } x_0 = 1\\\\\n",
    "            &= w^\\top x.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Our goal here is to find the best value for $w$. So, this becomes an optimization problem. As this is a optimization problem, we have to define a cost function with respect whom we can figure out whether our choice is best or not. One possible solution is measuring the `Euclidean distances` between the target values and the predicted outputs. So we are defining our cost function like this:\n",
    "\n",
    "$$\n",
    "E(w) = \\sum_{i=1}^N \\Big( f(x_i; w_i) - t_i \\Big)^2\n",
    "$$\n",
    "\n",
    "## III.I. Least Squares\n",
    "Least squres is the simplest linear regression model. The parameter that gives best fit here will be\n",
    "\n",
    "$$\n",
    "w^* = \\arg\\min_w \\sum_{i=1}^{N} \\Big( f(x_i; w) - t_i \\Big)^2\n",
    "$$\n",
    "\n",
    "Here the error funciton is quadratic. So this problem can be analytically solved by setting derivative with respect to $w$ to zero. Let's consider, the target values are collected in matrix $t$, and the input samples are in matrix $X$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "t &= [t_1, t_2, \\cdots, t_N]^\\top \\\\ \n",
    "\\\\\n",
    "w &= [w_0, w_1, \\cdots, w_D]^\\top \\\\\n",
    "\\\\\n",
    "X &= \\begin{bmatrix}\n",
    "    x_{10} & x_{11} & x_{12} & \\dots  & x_{1D} \\\\\n",
    "    x_{20} & x_{21} & x_{22} & \\dots  & x_{2D} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{N0} & x_{N1} & x_{N2} & \\dots  & x_{ND}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "where the first column is one's, $x_{*0} = [1, 1, \\dots, 1]^\\top$.\n",
    "\n",
    "With this matrix, $f(x; w)$ can be written in matrix form as:\n",
    "$$\n",
    "f(x; w) = X w.\n",
    "$$\n",
    "Thus, the error function will become\n",
    "$$\n",
    "\\begin{align}\n",
    "E(w) &= \\sum_{i=1}^N \\Big(f(x_i; w_i) - t_i \\Big)^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, by doing the partial derivative w.r.t. $w$, we get - \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E(w)}{\\partial w} &= 2 X^\\top X w - 2 X^\\top t \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By setting this to zero, \n",
    "$$\n",
    "\\begin{align}\n",
    " 2 X^\\top X w - 2 X^\\top t  &= 0\\\\\n",
    "\\\\\n",
    "w &= \\big(X^\\top X\\big)^{-1} X^\\top t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, given the feature matrix $X$ and our weight vector $w$, least squares find the best fitted line by this formula.\n",
    "\n",
    "## III.II. Least Mean Squares\n",
    "In `Least Squares` we observed that, it uses all the training data in finding the best fit. This is a computationally costly operation when dealing with learge features or number of entried. When data is sufficiently large, we can consider `Least Mean Squares` which learn *sequentially* instead of what doing by `Least Squares`. During the *sequential* learning process, $w$ is updated for a single data point in each iteration.\n",
    "\n",
    "`Least Mean Squares` starts with an initial guess $w$ and changes it as it reads more data until it converges.\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla E_k \n",
    "$$\n",
    "\n",
    "Here $k$ represents the steps for the repetition and $E_k$ is the error for the $k$'th sample and $\\alpha$ is a learning rate. In the $k$'th iteration with the sample $x_k$, the gradient for the sum-of-squares error is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla E_k = \\frac{\\partial E}{\\partial w^{(k)}} &= \\frac{\\partial }{\\partial w^{(k)}}\\Big( f(x_k; w^{(k)}) - t_k \\Big)^2 \\\\\n",
    "&= 2\\Big( {w^{(k)}}^\\top x_k - t_k \\Big) x_k.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By replacing $\\nabla E_k$, we get:\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\Big( {w^{(k)}}^\\top x_k - t_k \\Big) x_k.\n",
    "$$\n",
    "\n",
    "So, given the feature matrix $X$ and the randomly initialized weight vector $w$; in each iteration of data points, `Least Mean Squares` update the $w$ by this formula.\n",
    "\n",
    "`Least Mean Squares` solves the potential computational issue of `Least Squares`, but it could be possible that this algorithm will never converge. For dealing this, we usually run this algorithm withing a fixed number of iterations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III.I. Super Classs Definition\n",
    "\n",
    "These are the super class definitions for general linear model (i.e. `LinearModel`). This is the base model for the implementation of both the models (i.e. `Least Squares Model` and `Least Mean Square Model`). The abstract methods (i.e. `train` and `use`) will enable the unified interfaces for child classes to be overridden.\n",
    "\n",
    "`LinearModel` also contains the implementations of two helper functions `_check_matrix` and `add_ones` which helps checking dimentional constraint and adding a basis to the featur matrix respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Super class for machine learning models \n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, X, T):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class LinearModel(BaseModel):\n",
    "    \"\"\"\n",
    "        Abstract class for a linear model \n",
    "        \n",
    "        Attributes\n",
    "        ==========\n",
    "        w       ndarray\n",
    "                weight vector/matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            weight vector w is initialized as None\n",
    "        \"\"\"\n",
    "        self.w = np.zeros([1,1])\n",
    "        # self.w = None\n",
    "\n",
    "    # check if the matrix is 2-dimensional. if not, raise an exception    \n",
    "    def _check_matrix(self, mat, name):\n",
    "        if len(mat.shape) != 2:\n",
    "            raise ValueError(''.join([\"Wrong matrix \", name]))\n",
    "        \n",
    "    # add a basis\n",
    "    def add_ones(self, X):\n",
    "        \"\"\"\n",
    "            add a column basis to X input matrix\n",
    "        \"\"\"\n",
    "        self._check_matrix(X, 'X')\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    ####################################################\n",
    "    #### abstract funcitons ############################\n",
    "    @abstractmethod\n",
    "    def train(self, X, T):\n",
    "        \"\"\"\n",
    "            train linear model\n",
    "            \n",
    "            parameters\n",
    "            -----------\n",
    "            X     2d array\n",
    "                  input data\n",
    "            T     2d array\n",
    "                  target labels\n",
    "        \"\"\"        \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        \"\"\"\n",
    "            apply the learned model to input X\n",
    "            \n",
    "            parameters\n",
    "            ----------\n",
    "            X     2d array\n",
    "                  input data\n",
    "            \n",
    "        \"\"\"        \n",
    "        pass "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inheriting the super class, we can define two difference classes for Least Squares and Least Mean Square algorithms. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III.II. Least Squares\n",
    "\n",
    "`LinearRegress` class implements the least square solutions. Here, I implemented the `train` and `use` functions. `train()` updates the weights using least squares solution and `use()` returns the predictions for the argument X by using the trained weight w."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Linear Regression Class for least squares\n",
    "class LinearRegress(LinearModel): \n",
    "    \"\"\" \n",
    "        LinearRegress class \n",
    "        \n",
    "        attributes\n",
    "        ===========\n",
    "        w    nd.array  (column vector/matrix)\n",
    "             weights\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        LinearModel.__init__(self)\n",
    "        \n",
    "    # train lease-squares model\n",
    "    def train(self, X, T):\n",
    "        # First creast X1 by adding 1's column to X\n",
    "        N = X.shape[0]\n",
    "        X1 = self.add_ones(X)\n",
    "        # Next, using inverse, solve, lstsq function to get w*\n",
    "        self.w = np.linalg.inv(X1.transpose().dot(X1)).dot(X1.transpose()).dot(T)\n",
    "    \n",
    "    # apply the learned model to data X\n",
    "    def use(self, X):\n",
    "        # First creast X1 by adding 1's column to X\n",
    "        N = X.shape[0]\n",
    "        X1 = self.add_ones(X)\n",
    "        return X1.dot(self.w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III.III. Least Mean Squares\n",
    "\n",
    "For online learning, I have implemented LMS algorithm here. `train_step()` function updats the weights for a single input vector $x$ and one target label. I update the weight vector with $0$ if it is not initialized or it mismatches the shape with the input vector $x$. As the name suggests, `train()` function simply call `train_step()` in a loop to learn incrementally for the batch data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import collections # for checking iterable instance\n",
    "\n",
    "# LMS class \n",
    "class LMS(LinearModel):\n",
    "    \"\"\"\n",
    "        Lease Mean Squares. online learning algorithm\n",
    "    \n",
    "        attributes\n",
    "        ==========\n",
    "        w        nd.array\n",
    "                 weight matrix\n",
    "        alpha    float\n",
    "                 learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        LinearModel.__init__(self)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    # batch training by using train_step function\n",
    "    def train(self, X, T):\n",
    "        for x, t in zip(X, T):\n",
    "            self.train_step(x, t)\n",
    "\n",
    "    # train LMS model one step \n",
    "    # here the x is 1d vector\n",
    "    def train_step(self, x, t):\n",
    "        N = x.shape[0]\n",
    "        X1 = np.hstack((np.ones(1),x))\n",
    "        N = X1.shape[0]\n",
    "        #print(self.w)\n",
    "        if(len(self.w)!= N):\n",
    "            self.w = np.zeros(N)\n",
    "        y = self.w @ X1\n",
    "        self.w -= self.alpha * (y - t) * X1\n",
    "       \n",
    "    \n",
    "    # apply the current model to data X\n",
    "    def use(self, X):\n",
    "        N = X.shape[0]\n",
    "        X1 = np.hstack((np.ones((N, 1)), X.reshape((X.shape[0], -1))))\n",
    "        y = X1 @ self.w\n",
    "        N = X1.shape[0]\n",
    "        y.shape = (N,1)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IV. Usage Examples\n",
    "\n",
    "In this section I used some sample linear data to test the integrary of my implementations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HERE follow are for my code tests.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IV.I. Usage Example: Least Squares\n",
    "\n",
    "First prepare the sample data and put it in $x$. Then call `LinearRegress` model to train and predict the data. Line plot indicate the integrary of the `LinearRegress` implementation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = np.linspace(0,10, 11).reshape((-1, 1))\n",
    "T = -2 * X + 3.2\n",
    "\n",
    "ls = LinearRegress()\n",
    "\n",
    "ls.train(X, T)\n",
    "\n",
    "plt.plot(ls.use(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IV.II. Usage Example: Least Mean Squares\n",
    "\n",
    "Here I used the same sample data $x$ and call `LMS` model to train and predict the data. In the first plotting, I draw the prediction lines in each `train_step`. And the later one showing the final prediction line after calling the `train` function. The line plots indicate the integrary of the `LMS` implementation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.1)\n",
    "for x, t in zip(X, T):\n",
    "    lms.train_step(x, t)\n",
    "    plt.plot(lms.use(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.1)\n",
    "lms.train(X, T)\n",
    "plt.plot(lms.use(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# V. Preliminary Test\n",
    "\n",
    "This section contains the integrary test of the implmentation of `LinearRegress` and `LMS` classes. As all the tests passed here, getting much confidence for running my implementations of linear regression models (i.e. `LinearRegress` and `LMS`) w.r.t. the `House Sales in King County, USA` data described in `section II`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##################### WHAT I WILL RELEASE ############\n",
    "\n",
    "# Self-Test code for accuracy of your model - DO NOT MODIFY THIS\n",
    "# Primilnary test data\n",
    "X = np.array([[2,5],\n",
    "              [6,2],\n",
    "              [1,9],\n",
    "              [4,5],\n",
    "              [6,3],\n",
    "              [7,4],\n",
    "              [8,3]])\n",
    "T = X[:,0, None] * 3 - 2 * X[:, 1, None] + 3\n",
    "N = X.shape[0]\n",
    "\n",
    "def rmse(T, Y):\n",
    "    return np.sqrt(np.sum((T-Y)**2))\n",
    "\n",
    "model_names = ['LS', 'LMS_All', 'LMS_1STEP']\n",
    "models = [LinearRegress(), LMS(0.02), LMS(0.02)]\n",
    "#train\n",
    "for i, model in enumerate(models):\n",
    "    print(\"training \", model_names[i], \"...\") \n",
    "    if i == len(models) -1: \n",
    "        # train only one step for LMS2\n",
    "        model.train_step(X[0], T[0])\n",
    "    else:\n",
    "        model.train(X, T)\n",
    "\n",
    "def check(a, b, eps=np.finfo(float).eps):\n",
    "    if abs(a-b) > eps:\n",
    "        print(\"failed.\", a, b)\n",
    "    else:\n",
    "        print(\"passed.\")\n",
    "\n",
    "errors = [1.19e-13, 2.8753214702, 38.0584918251]\n",
    "for i, model in enumerate(models):\n",
    "    print(\"---- Testing \", model_names[i], \"...\", end=\" \") \n",
    "    \n",
    "    # rmse test\n",
    "    err = rmse(T, model.use(X))\n",
    "    if check(err, errors[i], eps=1e-10):\n",
    "        print (\"check your weights: \", model.w)\n",
    "        print (\"oracle: \", )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VI. Experiments\n",
    "\n",
    "This section contains the experiment results of `LinearRegress` and `LMS` models (described in `section III`) w.r.t. the `House Sales in King County, USA` data (described in `section II`). Basically we will predict the `price` of the house considering the other features of the data (i.e. `sqft_living`, `view`, `grade`, etc.). My experiment domain is listed in the `Table 1` bellow.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Algorithms</th>\n",
    "      <th>Feature Selection</th>\n",
    "      <th>Feature Scaling</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td align=\"middle\">Least Squares<br><br>\n",
    "                           Least Mean Squares</td>\n",
    "      <td align=\"middle\">All Features<br><br>\n",
    "                           Significant Features</td>\n",
    "      <td align=\"middle\">Unscaled<br><br>\n",
    "                           Normalized<br><br>\n",
    "                           Standarized</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td colspan=\"3\" align=\"middle\">Table 1: Summary of the experiment domain.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "**Feature Scaling**\n",
    "\n",
    "Differentt scales of input variables may cause problem for the regression models. Having features on a similar scale can help the gradient descent converge more quickly towards the minima.\n",
    "\n",
    "* Dominated by one large scale inputs!\n",
    "* \"Standardize\" the range of independent input features\n",
    "\n",
    "There are different ways of doing the scaling. In this experiment I considered the following two feature scaling technique,\n",
    "* Min-Max Normalization (I used `MinMaxScaler` from the `sklearn.preprocessing` library)\n",
    "* Standardization (I used `StandardScaler` from the `sklearn.preprocessing` library)\n",
    "\n",
    "Before jumping to the experiment results, here is the detailed descriptions of the experiment subsections:\n",
    "\n",
    "**VI.I. Plotting Functions**\n",
    "\n",
    "Data visualization is crutial to understand the performance of ML models. In this section, I put general graph plotting functions that will help visualizing different performance matric. Particularly the `residual plotting` is a requirement (the third one) for the `Extra Credit` (please check `section VIII`).\n",
    "\n",
    "A residual is a measure of how far away a point is vertically from the prediction line. Basically, it is the error between a predicted value and the corresponding observed actual value.\n",
    "\n",
    "**VI.II. Data Partitioning**\n",
    "\n",
    "First we randomly shuffle the whole dataset, which gives two benifits,\n",
    "\n",
    "1. Improve the ML model quality\n",
    "2. Improve the predictive performance\n",
    "\n",
    "As we know, to test the performance of linear regession model we have to partition the data into `train-data` and `test-data`. Model training will be done on the `train-data` and then the performance of the model training is done on the `test-data`. As we have total `21613` data point, I considered `19000` data for training and the rest for testing.\n",
    "\n",
    "Besides partitioning, feature selection is also very crutial for regression models. In feature selection we actually determine which features are importent predicting the output. As discussed in `section II.III.I`, the most correlated attributes with the target attribute (i.e. `price`) are\n",
    "1. bathrooms\n",
    "2. sqft_living\n",
    "3. view\n",
    "4. grade\n",
    "5. sqft_above\n",
    "6. sqft_living15\n",
    "\n",
    "So considering this, I do two types of experiments,\n",
    "1. Selecting all the features\n",
    "2. Selecting the most significant features (I considered the features that have `correlation_val >= 0.5` as the significant features).\n",
    "\n",
    "I believe this covers the first two requirements of `Extra Credit` listed in `section VIII`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VI.I. Plotting Functions\n",
    "\n",
    "In this section, I put general graph plotting functions (snapshot of `Prediction Vs. Actual`, `feature scaling` comparator plotting, `residual plotting`) that will help visualizing different performance matric."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prediction comparator plotting\n",
    "\n",
    "    attributes\n",
    "    ==========\n",
    "    predicted     array of predicted values\n",
    "    actual        array of actual values\n",
    "    feature_type  feature selection\n",
    "\"\"\"\n",
    "def predict_comp_plotting(predicted, actual, feature_type):\n",
    "    fig = plt.figure(figsize=(10, 5), dpi = 150)\n",
    "    plt.plot(predicted, label=\"Prediction\")\n",
    "    plt.plot(actual, label=\"Actual\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0.83, 0.98), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.xlabel('Test data index')\n",
    "    plt.ylabel('Prediction Vs. Actual')\n",
    "    plt.title('Snapshot of Prediction Vs. Actual plotting for ' + feature_type, color = 'green', fontsize = 20)\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Feature scaling comparator plotting\n",
    "\n",
    "    attributes\n",
    "    ==========\n",
    "    predicted   array of predicted values\n",
    "    actual      array of actual values\n",
    "    columns     xticks of the box plot (feature title)\n",
    "    scale_type  feature scaling\n",
    "\"\"\"\n",
    "\n",
    "def feature_scal_comp_plotting(unscaled, scaled, columns, scale_type):\n",
    "    f, axes = plt.subplots(1, 2, figsize=(15,6))\n",
    "\n",
    "    plt_norm = sns.boxplot(data=pd.DataFrame(unscaled), orient='v', ax=axes[0])\n",
    "    axes[0].set_xticklabels(columns, rotation=90)\n",
    "    axes[0].set_title('Unscaled Data')\n",
    "\n",
    "    sns.boxplot(data=pd.DataFrame(scaled), orient='v', ax=axes[1])\n",
    "    axes[1].set_xticklabels(columns, rotation=90)\n",
    "    axes[1].set_title(scale_type + ' Data')\n",
    "\n",
    "    f.suptitle('Comparing Unscaled Vs. ' + scale_type + ' data', color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Residual plotting\n",
    "\n",
    "    attributes\n",
    "    ==========\n",
    "    predicted     array of predicted values\n",
    "    actual        array of actual values\n",
    "    feature_type  feature selection\n",
    "\"\"\"\n",
    "\n",
    "def residual_plotting(predicted, actual, feature_type):\n",
    "    diff = actual - predicted\n",
    "    sample_count = np.arange(len(actual))\n",
    "    fig=plt.figure(figsize=(10, 5), dpi= 144, facecolor='w', edgecolor='k')\n",
    "    plt.scatter(sample_count, diff, marker='.')\n",
    "    plt.plot([0, 2600],[0, 2600], 'r-')\n",
    "    plt.title('Residual Plot for ' + feature_type, color = 'green', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VI.II. Data Partitioning\n",
    "\n",
    "First we randomly shuffle the whole dataset. And then partition the data into `train-data` and `test-data` samples. As we have total 21613 data points, I considered 19000 data for training and the rest for testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# randomly shuffel all data\n",
    "df_r.sample(frac=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# list of all features\n",
    "all_features = ['long', 'lat', \n",
    "          'yr_renovated', 'yr_built', \n",
    "          'sqft_basement', 'sqft_above', \n",
    "          'sqft_lot', 'sqft_lot15',\n",
    "          'sqft_living', 'sqft_living15',\n",
    "          'condition', 'view', 'grade', \n",
    "          'floors', 'waterfront',\n",
    "          'zipcode', \n",
    "          'bedrooms', 'bathrooms']\n",
    "\n",
    "# list of significant features\n",
    "sig_features = ['sqft_living', 'sqft_living15', 'sqft_above', 'view', 'grade', 'bathrooms']\n",
    "\n",
    "# list of target features\n",
    "target_features = ['price']\n",
    "\n",
    "# feature scaling type\n",
    "scale_type_norm = 'Normalized'\n",
    "scale_type_stand = 'Standarized'\n",
    "\n",
    "# feature selection type\n",
    "feature_type_all = \"All Features\"\n",
    "feature_type_sig = \"Significant Features\"\n",
    "\n",
    "# training data partition threshold\n",
    "train_data_th = 19000\n",
    "snapshot_th = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.II.I. Unscaled Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# partitioning unscaled-data of all possible features\n",
    "\n",
    "X_All = df_r[all_features].copy()\n",
    "\n",
    "X_All_Train = np.array(X_All.iloc[:train_data_th])\n",
    "X_All_Test = np.array(X_All.iloc[train_data_th:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# partitioning unscaled-data of most significant co-related features (i.e. correlation_val >= 0.5)\n",
    "\n",
    "X = df_r[sig_features].copy()\n",
    "X_Train = np.array(X.iloc[:train_data_th])\n",
    "X_Test = np.array(X.iloc[train_data_th:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# partitioning unscaled-data of target features\n",
    "\n",
    "T = df_r[target_features].copy()\n",
    "T_Train = np.array(T.iloc[:train_data_th])\n",
    "T_Test = np.array(T.iloc[train_data_th:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.II.II. Normalized Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# partitioning normalized-data of all possible features\n",
    "\n",
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_All_Train_Norm = np.array(X_All.iloc[:train_data_th])\n",
    "X_All_Test_Norm = np.array(X_All.iloc[train_data_th:])\n",
    "\n",
    "# normalize all possible feature data\n",
    "norm = MinMaxScaler().fit(X_All_Train)\n",
    "X_All_Train_Norm = norm.transform(X_All_Train)\n",
    "X_All_Test_Norm = norm.transform(X_All_Test)\n",
    "\n",
    "feature_scal_comp_plotting(X_All_Train, X_All_Train_Norm, all_features, scale_type_norm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# partitioning normalized-data of most significant co-related features (i.e. correlation_val >= 0.5)\n",
    "\n",
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_Train_Norm = np.array(X.iloc[:train_data_th])\n",
    "X_Test_Norm = np.array(X.iloc[train_data_th:])\n",
    "\n",
    "# normalize significant feature data\n",
    "norm = MinMaxScaler().fit(X_Train)\n",
    "X_Train_Norm = norm.transform(X_Train)\n",
    "X_Test_Norm = norm.transform(X_Test)\n",
    "\n",
    "feature_scal_comp_plotting(X_Train, X_Train_Norm, sig_features, scale_type_norm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.II.II. Standardized Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# partitioning standarized-data of all possible features\n",
    "\n",
    "# data standardization with  sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_All_Train_Stand = np.array(X_All.iloc[:train_data_th])\n",
    "X_All_Test_Stand = np.array(X_All.iloc[train_data_th:])\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# standarized all possible feature data\n",
    "X_All_Train_Stand = scaler.fit_transform(X_All_Train)\n",
    "X_All_Test_Stand = scaler.fit_transform(X_All_Test)\n",
    "\n",
    "feature_scal_comp_plotting(X_All_Train, X_All_Train_Stand, all_features, scale_type_stand)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# partitioning standarized-data of most significant co-related features (i.e. correlation_val >= 0.5)\n",
    "\n",
    "# data standardization with  sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_Train_Stand = np.array(X.iloc[:train_data_th])\n",
    "X_Test_Stand = np.array(X.iloc[train_data_th:])\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# standarized significant feature data\n",
    "X_Train_Stand = scaler.fit_transform(X_Train)\n",
    "X_Test_Stand = scaler.fit_transform(X_Test)\n",
    "\n",
    "feature_scal_comp_plotting(X_Train, X_Train_Stand, sig_features, scale_type_stand)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VI.III. Experiments on Least Squares\n",
    "\n",
    "This section contains the experiment results on `Least Squares` algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.III.I. Feature Scaling: Unscaled Data\n",
    "\n",
    "Here is the `Least Squares` algorithm's performance evaliation on unscaled data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.III.I.I. Feature Selection: All Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LinearRegress()\n",
    "lr.train(X_All_Train, T_Train)\n",
    "T_Predicted = lr.use(X_All_Test)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_all)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.III.I.II. Feature Selection: Significant Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LinearRegress()\n",
    "lr.train(X_Train, T_Train)\n",
    "T_Predicted = lr.use(X_Test)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_sig)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_sig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:**\n",
    "\n",
    "From the rmse value (`464727090` for all the features Vs. `12318249` for significant features) and the residual plotting we can understand the importance of removing `insignificant features`. The rmse value gets better by `38x` when we considered only the significant features.\n",
    "\n",
    "From the residual plot, we can observe that the data points get more clustered on the prediction line. Which also support the evidence we get from the change of rmse value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.III.II. Feature Scaling: Normalizaed Data\n",
    "\n",
    "Here is the `Least Squares` algorithm's performance evaliation on normalized data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.III.II.I. Feature Selection: All Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LinearRegress()\n",
    "lr.train(X_All_Train_Norm, T_Train)\n",
    "T_Predicted = lr.use(X_All_Test_Norm)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_all)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.III.II.II. Feature Selection: Significant Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LinearRegress()\n",
    "lr.train(X_Train_Norm, T_Train)\n",
    "T_Predicted = lr.use(X_Test_Norm)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_sig)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_sig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:**\n",
    "\n",
    "From the rmse value (`40098628` for all the features Vs. `12318249` for significant features) we again gets the proof of the importance of removing `insignificant features`. The residual plot also supports this.\n",
    "\n",
    "Although, the performance gap is reduced comparing to the experiment result of the unscaled data (`38x` Vs. `3.25`). I believe this change occurs as we `normalized` the input feature data. Interestingly, we also can observe that for all the features `normalization` improves the performance by `11.5`, but there is no such observation found in the significant feature data. I believe this is because the impacts of normalization have been dominated by the signigicant features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.III.III. Feature Scaling: Standarized Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.III.III.I. Feature Selection: All Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LinearRegress()\n",
    "lr.train(X_All_Train_Stand, T_Train)\n",
    "T_Predicted = lr.use(X_All_Test_Stand)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_all)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.III.III.II. Feature Selection: Significant Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LinearRegress()\n",
    "lr.train(X_Train_Stand, T_Train)\n",
    "T_Predicted = lr.use(X_Test_Stand)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_sig)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_sig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:**\n",
    "\n",
    "From the rmse value (`11513128` for all the features Vs. `12533302` for significant features) we again gets the proof of the importance of removing `insignificant features`. The residual plot also supports this. Interestingly, the performance gap indicate that the performance of the algorithm degrades while selecting significant features on the standarized data. I believe this change occurs as we `standarided` the input feature data and the impacts of standaraization have been dominated here due to feature selection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VI.IV. Experiments on Least Mean Squares (LMS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.IV.I. Feature Scaling: Unscaled Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.IV.I.I. Feature Selection: All Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.02)\n",
    "lms.train(X_All_Train, T_Train)\n",
    "T_Predicted = lms.use(X_All_Test)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.IV.I.II. Feature Selection: Significant Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.02)\n",
    "lms.train(X_Train, T_Train)\n",
    "T_Predicted = lms.use(X_Test)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We got `overflow encountered in multiply` while running the `Least Mean Squares` on the unscaled data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.IV.II. Feature Scaling: Normalized Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.IV.II.I. Feature Selection: All Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.02)\n",
    "lms.train(X_All_Train_Norm, T_Train)\n",
    "T_Predicted = lms.use(X_All_Test_Norm)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_all)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.IV.II.II. Feature Selection: Significant Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.02)\n",
    "lms.train(X_Train_Norm, T_Train)\n",
    "T_Predicted = lms.use(X_Test_Norm)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:100], T_Test[:100], feature_type_sig)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_sig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI.IV.III. Feature Scaling: Standarized Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.IV.III.I. Feature Selection: All Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.02)\n",
    "lms.train(X_All_Train_Stand, T_Train)\n",
    "T_Predicted = lms.use(X_All_Test_Stand)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:snapshot_th], T_Test[:snapshot_th], feature_type_all)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VI.IV.III.II. Feature Selection: Significant Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lms = LMS(0.02)\n",
    "lms.train(X_Train_Stand, T_Train)\n",
    "T_Predicted = lms.use(X_Test_Stand)\n",
    "\n",
    "print(\"rmse value\", rmse(T_Test, T_Predicted))\n",
    "predict_comp_plotting(T_Predicted[:100], T_Test[:100], feature_type_sig)\n",
    "residual_plotting(T_Predicted, T_Test, feature_type_sig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observation**\n",
    "\n",
    "* Normalization works better than standarization.\n",
    "* Feature scaling makes important difference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VI.V. Experiment Results\n",
    "\n",
    "In this section I like to give a recap about the overall experiment result. `Table-2` bellow gives the overall rmse values and `Table-3` represents the comparative performance (w.r.t. the rmse value).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Table 2: Summary of the RMSE value within the experiment domain.</th>\n",
    "        <th>Table 3: Comparison of RMSE value within the experiment domain.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Algorithms</th>\n",
    "                    <th>Feature Selection</th>\n",
    "                    <th colspan=\"3\" align=\"middle\">Feature Scaling</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th></th>\n",
    "                    <th></th>\n",
    "                    <th>Unscaled</th>\n",
    "                    <th>Normalized</th>\n",
    "                    <th>Standarized</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"2\">Least Squares</th>\n",
    "                    <th>All Features</th>\n",
    "                    <td>464727090</td>\n",
    "                    <td>40098628</td>\n",
    "                    <td>11513129</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Significant Features</th>\n",
    "                    <td>12318249</td>\n",
    "                    <td>12318249</td>\n",
    "                    <td>12533303</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"2\">Least Mean Squares</th>\n",
    "                    <th>All Features</th>\n",
    "                    <td>N/A</td>\n",
    "                    <td>11021941</td>\n",
    "                    <td>3.25e+39</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Significant Features</th>\n",
    "                    <td>N/A</td>\n",
    "                    <td>12703794</td>\n",
    "                    <td>13333369</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                  <td colspan=\"5\" align=\"middle\"></td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </td>\n",
    "        <td>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Algorithms</th>\n",
    "                    <th>Feature Selection</th>\n",
    "                    <th colspan=\"3\" align=\"middle\">Feature Scaling</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th></th>\n",
    "                    <th></th>\n",
    "                    <th>Unscaled</th>\n",
    "                    <th>Normalized</th>\n",
    "                    <th>Standarized</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"2\">Least Squares</th>\n",
    "                    <th>All Features</th>\n",
    "                    <td>37.70 X</td>\n",
    "                    <td>3.25 X</td>\n",
    "                    <td>0.94 X</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Significant Features</th>\n",
    "                    <td>X</td>\n",
    "                    <td>X</td>\n",
    "                    <td>1.02 X</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"2\">Least Mean Squares</th>\n",
    "                    <th>All Features</th>\n",
    "                    <td>N/A</td>\n",
    "                    <td>X</td>\n",
    "                    <td>2.95e32 X</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Significant Features</th>\n",
    "                    <td>N/A</td>\n",
    "                    <td>1.15 X</td>\n",
    "                    <td>1.20 X</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                  <td colspan=\"5\" align=\"middle\"></td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The above tables indicate that standarization within all the features gives the best result for `Least Squares`. On the other hand, normalization within all the features gives the best result for `Least Mean Squares`. It also indicates that the feature scaling improves the performance of `Least Squares` algorithm; but as I didn't able to get the result for `Least Mean Squares` on unscaled data, it was not possible to observe such behaviour. But it was surprising that instead of improving the performance, selecting significant features degrades the performance in `Least Mean Squares`. Also, the rmse value for `Least Mean Squares` on the standarized data for all the features are quite high, but the residual plotting indicates somewhat best performance. Due to the time constraint, I didn't go further to investigate these issues. Instead, I kept these as the future work for this assignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VII. Conclusions\n",
    "\n",
    "In this assignment I have performed linear regression on the `House Sales in King County, USA` using the `Linear Square` and `Linear Mean Square` methods. The `Linear Mean Square` is more sophisticated (as I get exception on the unscaled data) due to the greater detail in its computation compared to the `Linear Square` model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VIII. Extra Credit\n",
    "\n",
    "If you want to work more for an extra credit, place your work here for additional analysis: weight and residual analysis. \n",
    "Try to answer to the following questions: \n",
    "1. what is the most and least significant features for your data. (covered in `section VI. Experiments`)\n",
    "2. what are the consequences if you remove those features from the model? (covered in `section VI., VI.III.II., VI.III.III., VI.IV.II., VI.IV.III.`)\n",
    "3. produce residual plots and observe the patterns for the goodness of fit (covered in `section VI., VI.III.II., VI.III.III., VI.IV.II., VI.IV.III.`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IX. References\n",
    "\n",
    "1. harlfoxem (2016). House Sales in King County, USA, Version 1. Retrieved September 20, 2020 from https://www.kaggle.com/harlfoxem/housesalesprediction.\n",
    "\n",
    "2. Kaggle: Your Machine Learning and Data Science Community, https://www.kaggle.com, accessed Oct. 1, 2020."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}